from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score
from sklearn.cluster import DBSCAN
from googleapiclient.discovery import build
from collections import defaultdict
import numpy as np
import umap
import matplotlib.pyplot as plt
from tqdm import tqdm

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
YOUTUBE_API_KEY = 'AIzaSyB2m6IUTzz94_tMFAuhOy-ZQUtknCfbLQM'  # ‚Üê –ó–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π –∫–ª—é—á
VIDEO_ID = '8Fv6fJXjxao'               # ‚Üê –ó–∞–º–µ–Ω–∏ –Ω–∞ –Ω—É–∂–Ω—ã–π ID
MAX_COMMENTS = 450
PCA_DIM = 50
EPS_GRID = np.linspace(0.2, 3.0, 30)
MIN_SAMPLES = 17  # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ min_samples

# === –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ ===
def get_youtube_comments(api_key, video_id, max_comments=100):
    youtube = build('youtube', 'v3', developerKey=api_key)
    comments = []
    next_page_token = None

    while len(comments) < max_comments:
        response = youtube.commentThreads().list(
            part='snippet',
            videoId=video_id,
            maxResults=min(100, max_comments - len(comments)),
            textFormat='plainText',
            pageToken=next_page_token
        ).execute()

        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)

        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break

    return comments

# === –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ ===
comments = get_youtube_comments(YOUTUBE_API_KEY, VIDEO_ID, MAX_COMMENTS)
if not comments:
    print("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
    exit()

# === –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + PCA ===
print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏...")
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = model.encode(comments, show_progress_bar=True)
normalized_embeddings = normalize(embeddings)

pca = PCA(n_components=PCA_DIM)
reduced_embeddings = pca.fit_transform(normalized_embeddings)

# === –ü–æ–¥–±–æ—Ä eps —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º min_samples ===
print(f"–ü–æ–∏—Å–∫ eps —Å min_samples={MIN_SAMPLES}, silhouette_score –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–∫–∏–º –∫ 0.5 (–Ω–æ –Ω–µ –≤—ã—à–µ)...")
best_score = -999
best_eps = None
best_labels = None
target_score = 0.5

for eps in tqdm(EPS_GRID, desc="–ü–µ—Ä–µ–±–æ—Ä eps"):
    db = DBSCAN(eps=eps, min_samples=MIN_SAMPLES, metric='euclidean')
    labels = db.fit_predict(reduced_embeddings)
    mask = labels != -1
    n_clusters = len(set(labels[mask]))

    if n_clusters >= 2 and np.any(mask):
        try:
            score = silhouette_score(reduced_embeddings[mask], labels[mask])
            if score <= target_score and (best_score == -999 or abs(score - target_score) < abs(best_score - target_score)):
                best_score = score
                best_eps = eps
                best_labels = labels
        except:
            continue

if best_labels is None:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π eps, –≤—ã–±–∏—Ä–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π silhouette score –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è 0.5.")
    best_score = -1
    for eps in tqdm(EPS_GRID, desc="–ü–µ—Ä–µ–±–æ—Ä eps fallback"):
        db = DBSCAN(eps=eps, min_samples=MIN_SAMPLES, metric='euclidean')
        labels = db.fit_predict(reduced_embeddings)
        mask = labels != -1
        n_clusters = len(set(labels[mask]))

        if n_clusters >= 2 and np.any(mask):
            try:
                score = silhouette_score(reduced_embeddings[mask], labels[mask])
                if score > best_score:
                    best_score = score
                    best_eps = eps
                    best_labels = labels
            except:
                continue

print(f"\n‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
print(f"  eps = {best_eps:.2f}")
print(f"  min_samples = {MIN_SAMPLES}")
print(f"  Silhouette Score = {best_score:.3f}")
labels = best_labels

# === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è UMAP ===
print("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ UMAP...")
umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
embedding_2d = umap_reducer.fit_transform(reduced_embeddings)

# === –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ===
plt.figure(figsize=(12, 8))
unique_labels = set(labels)
colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))

for label, color in zip(unique_labels, colors):
    idx = labels == label
    label_name = f"–ö–ª–∞—Å—Ç–µ—Ä {label}" if label != -1 else "–®—É–º"
    plt.scatter(
        embedding_2d[idx, 0], embedding_2d[idx, 1],
        label=label_name,
        alpha=0.6,
        s=40,
        c=[color]
    )

plt.title("–ö–ª–∞—Å—Ç–µ—Ä—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (UMAP + DBSCAN)")
plt.legend(loc='best', fontsize='small')
plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.grid(True)
plt.tight_layout()
plt.show()

# === –í—ã–≤–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ===
clusters = defaultdict(list)
for comment, label in zip(comments, labels):
    clusters[label].append(comment)

print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
for label, cluster_comments in clusters.items():
    if label == -1:
        continue
    print(f"\n–ö–ª–∞—Å—Ç–µ—Ä {label} ‚Äî {len(cluster_comments)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤")
    for c in cluster_comments[:5]:
        print(f"  - {c}")

# === –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ===
total = len(comments)
outliers = len(clusters.get(-1, []))
n_clusters = len(set([l for l in labels if l != -1]))
print(f"\nüìä –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {n_clusters}")
print(f"üßπ –í—ã–±—Ä–æ—Å–æ–≤: {outliers} –∏–∑ {total} ({outliers / total:.1%})")
print(f"üìà –õ—É—á—à–∏–π Silhouette Score: {best_score:.3f}")